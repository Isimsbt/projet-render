import os
import logging
import cv2
import numpy as np
from fer import FER
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

# Configuration de logging
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppression des warnings TensorFlow
logging.basicConfig(level=logging.INFO)

# Initialisation de l'application FastAPI
app = FastAPI()

# Initialisation du d√©tecteur d'√©motions
detector = None

# √âv√©nements de d√©marrage et arr√™t
@app.on_event("startup")
async def startup_event():
    global detector
    try:
        detector = FER(mtcnn=True)
        logging.info("‚úÖ Mod√®le FER charg√© avec succ√®s")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors du chargement du mod√®le : {str(e)}")
        raise RuntimeError("Erreur lors du chargement du mod√®le FER")

@app.on_event("shutdown")
async def shutdown_event():
    logging.info("üõë API arr√™t√©e proprement")

# Mod√®les Pydantic pour la r√©ponse
class EmotionBox(BaseModel):
    x: int
    y: int
    width: int
    height: int

class FaceEmotion(BaseModel):
    box: EmotionBox
    dominant_emotion: str
    score: float
    all_emotions: dict

class EmotionResponse(BaseModel):
    emotions: List[FaceEmotion]
    message: Optional[str] = None

# Route d'accueil
@app.get("/", response_model=dict)
async def root():
    return {"message": "Bienvenue sur l'API de d√©tection d'√©motions!"}

# Route de d√©tection d'√©motions
@app.post("/detect_emotion", response_model=EmotionResponse)
async def detect_emotion(file: UploadFile = File(...)):
    """D√©tecte les √©motions dans une image t√©l√©charg√©e"""

    # V√©rification du format du fichier
    if not file.content_type.startswith('image/'):
        raise HTTPException(400, "Seules les images sont accept√©es")

    try:
        # Lecture de l'image
        image_data = await file.read()
        nparr = np.frombuffer(image_data, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if frame is None:
            raise HTTPException(400, "Impossible de d√©coder l'image")

        # Conversion en RGB pour le d√©tecteur FER
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = detector.detect_emotions(frame_rgb)

        # Formatage des r√©sultats
        emotions = []
        for face in results:
            box = EmotionBox(**dict(zip(['x', 'y', 'width', 'height'], face['box'])))
            emotions_dict = face['emotions']
            dominant = max(emotions_dict, key=emotions_dict.get)
            emotions.append(FaceEmotion(
                box=box,
                dominant_emotion=dominant,
                score=emotions_dict[dominant],
                all_emotions=emotions_dict
            ))

        return EmotionResponse(emotions=emotions, message="D√©tection r√©ussie")

    except Exception as e:
        logging.error(f"Erreur lors de la d√©tection : {str(e)}", exc_info=True)
        raise HTTPException(500, "Erreur interne du serveur")

# Test avec la webcam (local uniquement)
def test_webcam():
    """Test local avec la webcam - Non expos√© via l'API"""
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        logging.error("Impossible d'acc√©der √† la webcam")
        return

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = detector.detect_emotions(frame_rgb)

            for face in results:
                (x, y, w, h) = face['box']
                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

                emotions = face['emotions']
                dominant = max(emotions, key=emotions.get)
                score = emotions[dominant]

                label = f"{dominant} ({score:.1%})"
                cv2.putText(frame, label, (x, y - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8,
                            (0, 0, 255), 2)

            cv2.imshow('D√©tection en temps r√©el (Appuyez sur Q pour quitter)', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()

# Point d'entr√©e principal
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="API de d√©tection d'√©motions")
    parser.add_argument("--webcam", action="store_true", help="Lancer le test webcam")
    args = parser.parse_args()

    if args.webcam:
        test_webcam()
    else:
        # Lancement de l'API en local
                uvicorn.run(app, host="localhost",¬†port=8000)

import logging
import os
import cv2
import numpy as np
from fastapi import FastAPI, UploadFile, File, HTTPException
from fer import FER
import uvicorn

# Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Initialiser FastAPI
app = FastAPI(
    title="D√©tection des √©motions avec FER",
    description="API pour d√©tecter les √©motions dans une image en utilisant le mod√®le FER.",
    version="1.0.0"
)

# Charger le d√©tecteur FER avec MTCNN
detector = None
try:
    logger.info("üîÑ Chargement du mod√®le FER avec MTCNN...")
    detector = FER(mtcnn=True)
    logger.info("‚úÖ Mod√®le FER charg√© avec succ√®s.")
except Exception as e:
    logger.error(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
    raise RuntimeError("√âchec du chargement du mod√®le. V√©rifiez l'installation de `fer`.")

# Endpoint principal
@app.get("/", summary="Message d'accueil")
async def root():
    return {"message": "Bienvenue sur l'API de d√©tection des √©motions. Utilisez /predict pour envoyer une image."}

# Endpoint de v√©rification de l'√©tat
@app.get("/health", summary="V√©rification de l'√©tat de l'API")
async def health():
    return {"model_loaded": detector is not None, "status": "healthy"}

@app.post("/predict", summary="D√©tecter les √©motions dans une image")
async def predict_emotion(file: UploadFile = File(...)):
    """
    Accepte une image et renvoie les √©motions d√©tect√©es avec leurs annotations.
    """
    logger.info("üì∑ R√©ception d'une image pour analyse...")
    
    # V√©rification du format du fichier
    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="Le fichier doit √™tre une image valide.")

    try:
        # Lire l'image envoy√©e
        contents = await file.read()
        nparr = np.frombuffer(contents, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if frame is None:
            raise HTTPException(status_code=400, detail="Image invalide")

        # Convertir en RGB pour FER
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # D√©tecter les √©motions
        logger.info("üîç Analyse des √©motions...")
        result = detector.detect_emotions(frame_rgb)

        # Formater les r√©sultats
        if not result:
            logger.warning("‚ö†Ô∏è Aucun visage d√©tect√© dans l'image.")
            return {"message": "Aucun visage d√©tect√©", "emotions": []}

        formatted_result = []
        for face in result:
            # Coordonn√©es du visage
            (x, y, w, h) = face["box"]
            # Landmarks
            landmarks = face.get("keypoints", {})
            # √âmotions
            emotions = face["emotions"]
            dominant_emotion = max(emotions, key=emotions.get)
            score = emotions[dominant_emotion]

            formatted_result.append({
                "box": {"x": x, "y": y, "width": w, "height": h},
                "landmarks": {
                    "left_eye": landmarks.get("left_eye", [0, 0]),
                    "right_eye": landmarks.get("right_eye", [0, 0]),
                    "mouth_left": landmarks.get("mouth_left", [0, 0]),
                    "mouth_right": landmarks.get("mouth_right", [0, 0])
                },
                "emotions": emotions,
                "dominant_emotion": dominant_emotion,
                "score": float(score)
            })

        logger.info(f"‚úÖ √âmotion dominante d√©tect√©e : {dominant_emotion}")
        return {"emotions": formatted_result}

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'analyse de l'image : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur : {str(e)}")

# Lancement du serveur
if __name__ == "__main__":
    port = int(os.getenv("PORT", 10000))  # Utiliser la variable PORT de Render, avec 10000 comme fallback
    logger.info(f"üöÄ Lancement de l'API sur le port {port}...")
    uvicorn.run(app, host="0.0.0.0", port=port)